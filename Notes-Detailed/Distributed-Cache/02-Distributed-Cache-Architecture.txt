================================================================================
         DISTRIBUTED CACHE SYSTEM DESIGN
         Chapter 2: Distributed Cache Architecture
================================================================================

When a single cache server isn't enough, we need distributed caching.
This chapter covers how to distribute cache across multiple nodes while
maintaining performance and consistency.


================================================================================
SECTION 2.1: WHY DISTRIBUTED CACHE?
================================================================================

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                                         â”‚
    â”‚  SINGLE NODE LIMITATIONS                                               â”‚
    â”‚                                                                         â”‚
    â”‚  1. MEMORY LIMIT                                                       â”‚
    â”‚     â€¢ Single server: 64-256 GB RAM typical                           â”‚
    â”‚     â€¢ Need to cache 1 TB? Can't fit on one server                   â”‚
    â”‚                                                                         â”‚
    â”‚  2. THROUGHPUT LIMIT                                                   â”‚
    â”‚     â€¢ Single Redis: ~100,000 ops/sec                                 â”‚
    â”‚     â€¢ Need 1M ops/sec? Need more servers                            â”‚
    â”‚                                                                         â”‚
    â”‚  3. SINGLE POINT OF FAILURE                                            â”‚
    â”‚     â€¢ Server crashes = all cached data lost                          â”‚
    â”‚     â€¢ Thundering herd to database                                    â”‚
    â”‚                                                                         â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
    â”‚                                                                         â”‚
    â”‚  DISTRIBUTED CACHE BENEFITS                                            â”‚
    â”‚                                                                         â”‚
    â”‚  1. HORIZONTAL SCALING                                                 â”‚
    â”‚     â€¢ Add more nodes for more capacity                               â”‚
    â”‚     â€¢ 10 nodes Ã— 64GB = 640GB total cache                           â”‚
    â”‚                                                                         â”‚
    â”‚  2. HIGH AVAILABILITY                                                  â”‚
    â”‚     â€¢ Replicas survive node failures                                 â”‚
    â”‚     â€¢ No single point of failure                                     â”‚
    â”‚                                                                         â”‚
    â”‚  3. GEOGRAPHIC DISTRIBUTION                                            â”‚
    â”‚     â€¢ Cache in multiple regions                                      â”‚
    â”‚     â€¢ Lower latency for global users                                 â”‚
    â”‚                                                                         â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚                                                                 â”‚  â”‚
    â”‚  â”‚  SINGLE CACHE              DISTRIBUTED CACHE                   â”‚  â”‚
    â”‚  â”‚                                                                 â”‚  â”‚
    â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”         â”‚  â”‚
    â”‚  â”‚  â”‚           â”‚             â”‚Node 1â”‚ â”‚Node 2â”‚ â”‚Node 3â”‚         â”‚  â”‚
    â”‚  â”‚  â”‚  Single   â”‚             â”‚ 64GB â”‚ â”‚ 64GB â”‚ â”‚ 64GB â”‚         â”‚  â”‚
    â”‚  â”‚  â”‚   Node    â”‚      â†’      â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜         â”‚  â”‚
    â”‚  â”‚  â”‚  64 GB    â”‚             â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”         â”‚  â”‚
    â”‚  â”‚  â”‚           â”‚             â”‚Node 4â”‚ â”‚Node 5â”‚ â”‚Node 6â”‚         â”‚  â”‚
    â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚ 64GB â”‚ â”‚ 64GB â”‚ â”‚ 64GB â”‚         â”‚  â”‚
    â”‚  â”‚                            â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜         â”‚  â”‚
    â”‚  â”‚  Capacity: 64GB            Capacity: 384GB                    â”‚  â”‚
    â”‚  â”‚  Throughput: 100K/s        Throughput: 600K/s                â”‚  â”‚
    â”‚  â”‚                                                                 â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â”‚                                                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


================================================================================
SECTION 2.2: DATA DISTRIBUTION - CONSISTENT HASHING
================================================================================

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                                         â”‚
    â”‚  THE DISTRIBUTION PROBLEM                                             â”‚
    â”‚                                                                         â”‚
    â”‚  Given a key, which node stores it?                                   â”‚
    â”‚                                                                         â”‚
    â”‚  NAIVE APPROACH: MODULO HASHING                                       â”‚
    â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                       â”‚
    â”‚                                                                         â”‚
    â”‚  node_index = hash(key) % num_nodes                                   â”‚
    â”‚                                                                         â”‚
    â”‚  With 4 nodes:                                                         â”‚
    â”‚  hash("user:1") % 4 = 2  â†’ Node 2                                    â”‚
    â”‚  hash("user:2") % 4 = 0  â†’ Node 0                                    â”‚
    â”‚  hash("user:3") % 4 = 1  â†’ Node 1                                    â”‚
    â”‚                                                                         â”‚
    â”‚  PROBLEM: ADDING/REMOVING NODES                                       â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                          â”‚
    â”‚                                                                         â”‚
    â”‚  Add a 5th node:                                                       â”‚
    â”‚  hash("user:1") % 5 = 1  â†’ Node 1 (was Node 2!)                      â”‚
    â”‚  hash("user:2") % 5 = 3  â†’ Node 3 (was Node 0!)                      â”‚
    â”‚  hash("user:3") % 5 = 4  â†’ Node 4 (was Node 1!)                      â”‚
    â”‚                                                                         â”‚
    â”‚  Almost ALL keys move to different nodes!                            â”‚
    â”‚  Massive cache invalidation. ğŸ’¥                                       â”‚
    â”‚                                                                         â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
    â”‚                                                                         â”‚
    â”‚  CONSISTENT HASHING                                                    â”‚
    â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                    â”‚
    â”‚                                                                         â”‚
    â”‚  Imagine a circular ring (0 to 2^32-1)                                â”‚
    â”‚  Both nodes AND keys are hashed onto this ring.                      â”‚
    â”‚  Each key belongs to the first node clockwise from it.               â”‚
    â”‚                                                                         â”‚
    â”‚                        0                                               â”‚
    â”‚                        â”‚                                               â”‚
    â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
    â”‚              â”‚                   â”‚                                     â”‚
    â”‚       Node A â—        â— Key 1   â”‚                                     â”‚
    â”‚              â”‚                   â”‚                                     â”‚
    â”‚    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚
    â”‚              â”‚                   â”‚                                     â”‚
    â”‚       Key 3 â—         Node B â—  â”‚                                     â”‚
    â”‚              â”‚                   â”‚                                     â”‚
    â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
    â”‚                        â”‚                                               â”‚
    â”‚                â— Key 2 â”‚ â— Node C                                     â”‚
    â”‚                        â”‚                                               â”‚
    â”‚                                                                         â”‚
    â”‚  Key 1 â†’ Node B (first node clockwise)                               â”‚
    â”‚  Key 2 â†’ Node C                                                       â”‚
    â”‚  Key 3 â†’ Node A                                                       â”‚
    â”‚                                                                         â”‚
    â”‚  ADD A NODE (D between A and B):                                      â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                      â”‚
    â”‚  Only keys between A and D move to D.                                â”‚
    â”‚  Other keys stay where they are!                                     â”‚
    â”‚  Only ~1/N keys move (N = number of nodes).                          â”‚
    â”‚                                                                         â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
    â”‚                                                                         â”‚
    â”‚  VIRTUAL NODES                                                         â”‚
    â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•                                                          â”‚
    â”‚                                                                         â”‚
    â”‚  Problem: Uneven distribution with few nodes.                        â”‚
    â”‚                                                                         â”‚
    â”‚  Solution: Each physical node gets multiple positions on ring.       â”‚
    â”‚                                                                         â”‚
    â”‚  Node A â†’ A-1, A-2, A-3, A-4 (4 virtual nodes)                       â”‚
    â”‚  Node B â†’ B-1, B-2, B-3, B-4                                         â”‚
    â”‚                                                                         â”‚
    â”‚  More virtual nodes = more even distribution.                        â”‚
    â”‚  Typical: 100-200 virtual nodes per physical node.                   â”‚
    â”‚                                                                         â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
    â”‚                                                                         â”‚
    â”‚  IMPLEMENTATION                                                        â”‚
    â”‚                                                                         â”‚
    â”‚  class ConsistentHash:                                                 â”‚
    â”‚      def __init__(self, nodes, virtual_nodes=150):                   â”‚
    â”‚          self.ring = SortedDict()                                     â”‚
    â”‚          self.virtual_nodes = virtual_nodes                           â”‚
    â”‚                                                                         â”‚
    â”‚          for node in nodes:                                            â”‚
    â”‚              self.add_node(node)                                       â”‚
    â”‚                                                                         â”‚
    â”‚      def add_node(self, node):                                        â”‚
    â”‚          for i in range(self.virtual_nodes):                         â”‚
    â”‚              key = hash(f"{node}:{i}")                               â”‚
    â”‚              self.ring[key] = node                                    â”‚
    â”‚                                                                         â”‚
    â”‚      def remove_node(self, node):                                     â”‚
    â”‚          for i in range(self.virtual_nodes):                         â”‚
    â”‚              key = hash(f"{node}:{i}")                               â”‚
    â”‚              del self.ring[key]                                       â”‚
    â”‚                                                                         â”‚
    â”‚      def get_node(self, key):                                         â”‚
    â”‚          if not self.ring:                                            â”‚
    â”‚              return None                                               â”‚
    â”‚          hash_key = hash(key)                                         â”‚
    â”‚          # Find first node clockwise                                  â”‚
    â”‚          idx = self.ring.bisect_right(hash_key)                      â”‚
    â”‚          if idx == len(self.ring):                                   â”‚
    â”‚              idx = 0  # Wrap around                                   â”‚
    â”‚          return self.ring.peekitem(idx)[1]                           â”‚
    â”‚                                                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


================================================================================
SECTION 2.3: REPLICATION
================================================================================

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                                         â”‚
    â”‚  WHY REPLICATION?                                                      â”‚
    â”‚                                                                         â”‚
    â”‚  Node fails â†’ data lost â†’ cache miss spike â†’ database overload       â”‚
    â”‚                                                                         â”‚
    â”‚  With replication: Node fails â†’ replica takes over â†’ no data lost   â”‚
    â”‚                                                                         â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
    â”‚                                                                         â”‚
    â”‚  REPLICATION STRATEGIES                                                â”‚
    â”‚                                                                         â”‚
    â”‚  1. LEADER-FOLLOWER (Master-Slave)                                     â”‚
    â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                      â”‚
    â”‚                                                                         â”‚
    â”‚  â€¢ One leader (master) handles writes                                â”‚
    â”‚  â€¢ Followers (replicas) receive copies                               â”‚
    â”‚  â€¢ Reads can go to any node                                          â”‚
    â”‚                                                                         â”‚
    â”‚       Client                                                           â”‚
    â”‚          â”‚                                                             â”‚
    â”‚          â”‚ Write                                                       â”‚
    â”‚          â–¼                                                             â”‚
    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                        â”‚
    â”‚     â”‚ Leader  â”‚â”€â”€â”€â”€â–º Replicate â”€â”€â”€â”€â–º â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
    â”‚     â”‚ (Write) â”‚                       â”‚ Follower â”‚                     â”‚
    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”€â”€â”€â”€â–º Replicate â”€â”€â”€â”€â–º â”‚  (Read)  â”‚                     â”‚
    â”‚                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
    â”‚                 â”€â”€â”€â”€â–º Replicate â”€â”€â”€â”€â–º â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
    â”‚                                       â”‚ Follower â”‚                     â”‚
    â”‚                                       â”‚  (Read)  â”‚                     â”‚
    â”‚                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
    â”‚                                                                         â”‚
    â”‚  SYNC vs ASYNC REPLICATION:                                           â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                           â”‚
    â”‚  Sync: Leader waits for follower ack before confirming write        â”‚
    â”‚        Pros: Strong consistency                                       â”‚
    â”‚        Cons: Higher latency, leader blocked if follower slow        â”‚
    â”‚                                                                         â”‚
    â”‚  Async: Leader confirms immediately, replicates in background        â”‚
    â”‚         Pros: Fast writes                                             â”‚
    â”‚         Cons: Data loss possible if leader fails before replicate   â”‚
    â”‚                                                                         â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
    â”‚                                                                         â”‚
    â”‚  2. LEADERLESS REPLICATION                                             â”‚
    â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                            â”‚
    â”‚                                                                         â”‚
    â”‚  No single leader. Write to multiple nodes.                          â”‚
    â”‚  Read from multiple nodes, use quorum.                               â”‚
    â”‚                                                                         â”‚
    â”‚  QUORUM: W + R > N                                                    â”‚
    â”‚  â€¢ N = total replicas                                                â”‚
    â”‚  â€¢ W = write quorum (nodes that must ack write)                     â”‚
    â”‚  â€¢ R = read quorum (nodes to read from)                             â”‚
    â”‚                                                                         â”‚
    â”‚  Example: N=3, W=2, R=2                                               â”‚
    â”‚  Write succeeds when 2 nodes ack.                                    â”‚
    â”‚  Read from 2 nodes, at least 1 has latest.                          â”‚
    â”‚                                                                         â”‚
    â”‚  Used by: Cassandra, DynamoDB                                        â”‚
    â”‚                                                                         â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
    â”‚                                                                         â”‚
    â”‚  REDIS REPLICATION                                                     â”‚
    â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                      â”‚
    â”‚                                                                         â”‚
    â”‚  Redis uses async leader-follower by default.                        â”‚
    â”‚                                                                         â”‚
    â”‚  # On replica                                                          â”‚
    â”‚  REPLICAOF leader_ip leader_port                                      â”‚
    â”‚                                                                         â”‚
    â”‚  Replication lag: Typically <1ms, but can be seconds under load.    â”‚
    â”‚  WAIT command for sync replication if needed.                        â”‚
    â”‚                                                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


================================================================================
SECTION 2.4: REDIS CLUSTER
================================================================================

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                                         â”‚
    â”‚  REDIS CLUSTER                                                         â”‚
    â”‚                                                                         â”‚
    â”‚  Built-in sharding for Redis.                                        â”‚
    â”‚  Automatic data distribution and failover.                           â”‚
    â”‚                                                                         â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
    â”‚                                                                         â”‚
    â”‚  HASH SLOTS                                                            â”‚
    â”‚  â•â•â•â•â•â•â•â•â•â•                                                            â”‚
    â”‚                                                                         â”‚
    â”‚  Redis Cluster uses 16,384 hash slots.                               â”‚
    â”‚  Each key hashes to a slot: CRC16(key) % 16384                       â”‚
    â”‚  Slots distributed across nodes.                                      â”‚
    â”‚                                                                         â”‚
    â”‚  With 3 nodes:                                                         â”‚
    â”‚  â€¢ Node A: slots 0-5460                                              â”‚
    â”‚  â€¢ Node B: slots 5461-10922                                          â”‚
    â”‚  â€¢ Node C: slots 10923-16383                                         â”‚
    â”‚                                                                         â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚                                                                 â”‚  â”‚
    â”‚  â”‚   Node A              Node B              Node C               â”‚  â”‚
    â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚  â”‚
    â”‚  â”‚  â”‚ Slots  â”‚          â”‚ Slots  â”‚          â”‚ Slots  â”‚            â”‚  â”‚
    â”‚  â”‚  â”‚ 0-5460 â”‚          â”‚5461-   â”‚          â”‚10923-  â”‚            â”‚  â”‚
    â”‚  â”‚  â”‚        â”‚          â”‚10922   â”‚          â”‚16383   â”‚            â”‚  â”‚
    â”‚  â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜            â”‚  â”‚
    â”‚  â”‚       â”‚                   â”‚                   â”‚                â”‚  â”‚
    â”‚  â”‚       â–¼                   â–¼                   â–¼                â”‚  â”‚
    â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚  â”‚
    â”‚  â”‚  â”‚Replica â”‚          â”‚Replica â”‚          â”‚Replica â”‚            â”‚  â”‚
    â”‚  â”‚  â”‚  A1    â”‚          â”‚  B1    â”‚          â”‚  C1    â”‚            â”‚  â”‚
    â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚  â”‚
    â”‚  â”‚                                                                 â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â”‚                                                                         â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
    â”‚                                                                         â”‚
    â”‚  CLIENT ROUTING                                                        â”‚
    â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                        â”‚
    â”‚                                                                         â”‚
    â”‚  Client sends command to any node.                                   â”‚
    â”‚  If key is on that node: execute and return.                        â”‚
    â”‚  If key is on different node: return MOVED redirect.                â”‚
    â”‚                                                                         â”‚
    â”‚  GET user:123                                                          â”‚
    â”‚  â†’ MOVED 12345 192.168.1.3:6379                                      â”‚
    â”‚  (Key is in slot 12345, on node 192.168.1.3)                        â”‚
    â”‚                                                                         â”‚
    â”‚  Smart clients cache slotâ†’node mapping to avoid redirects.          â”‚
    â”‚                                                                         â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
    â”‚                                                                         â”‚
    â”‚  FAILOVER                                                              â”‚
    â”‚  â•â•â•â•â•â•â•â•                                                              â”‚
    â”‚                                                                         â”‚
    â”‚  1. Nodes ping each other constantly                                 â”‚
    â”‚  2. If master doesn't respond, followers vote                       â”‚
    â”‚  3. Elected follower promoted to master                             â”‚
    â”‚  4. Cluster updates slot mapping                                    â”‚
    â”‚                                                                         â”‚
    â”‚  Automatic failover: ~1-2 seconds.                                   â”‚
    â”‚                                                                         â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
    â”‚                                                                         â”‚
    â”‚  MULTI-KEY OPERATIONS                                                  â”‚
    â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                  â”‚
    â”‚                                                                         â”‚
    â”‚  MGET key1 key2 key3                                                  â”‚
    â”‚                                                                         â”‚
    â”‚  Only works if all keys are on same node.                           â”‚
    â”‚  Use hash tags to force co-location:                                 â”‚
    â”‚                                                                         â”‚
    â”‚  user:{123}:profile                                                    â”‚
    â”‚  user:{123}:settings                                                   â”‚
    â”‚  user:{123}:friends                                                    â”‚
    â”‚                                                                         â”‚
    â”‚  The part in {} is used for hashing.                                 â”‚
    â”‚  All keys with {123} go to same slot.                               â”‚
    â”‚                                                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


================================================================================
SECTION 2.5: REDIS SENTINEL (High Availability)
================================================================================

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                                         â”‚
    â”‚  REDIS SENTINEL                                                        â”‚
    â”‚                                                                         â”‚
    â”‚  Provides high availability without full clustering.                 â”‚
    â”‚  Good for smaller setups that need failover.                         â”‚
    â”‚                                                                         â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚                                                                 â”‚  â”‚
    â”‚  â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚  â”‚
    â”‚  â”‚           â”‚Sentinel 1â”‚  â”‚Sentinel 2â”‚  â”‚Sentinel 3â”‚             â”‚  â”‚
    â”‚  â”‚           â”‚  (S1)    â”‚  â”‚  (S2)    â”‚  â”‚  (S3)    â”‚             â”‚  â”‚
    â”‚  â”‚           â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜             â”‚  â”‚
    â”‚  â”‚                â”‚             â”‚             â”‚                    â”‚  â”‚
    â”‚  â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚  â”‚
    â”‚  â”‚                       Monitorâ”‚                                  â”‚  â”‚
    â”‚  â”‚                              â”‚                                  â”‚  â”‚
    â”‚  â”‚                              â–¼                                  â”‚  â”‚
    â”‚  â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚  â”‚
    â”‚  â”‚                        â”‚  Master  â”‚                             â”‚  â”‚
    â”‚  â”‚                        â”‚  (M)     â”‚                             â”‚  â”‚
    â”‚  â”‚                        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                             â”‚  â”‚
    â”‚  â”‚                             â”‚ Replication                       â”‚  â”‚
    â”‚  â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚  â”‚
    â”‚  â”‚                    â–¼                 â–¼                          â”‚  â”‚
    â”‚  â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚  â”‚
    â”‚  â”‚               â”‚ Replica 1â”‚      â”‚ Replica 2â”‚                    â”‚  â”‚
    â”‚  â”‚               â”‚  (R1)    â”‚      â”‚  (R2)    â”‚                    â”‚  â”‚
    â”‚  â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚  â”‚
    â”‚  â”‚                                                                 â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â”‚                                                                         â”‚
    â”‚  SENTINEL RESPONSIBILITIES:                                           â”‚
    â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                           â”‚
    â”‚                                                                         â”‚
    â”‚  1. MONITORING                                                         â”‚
    â”‚     â€¢ Sentinels ping master and replicas                            â”‚
    â”‚     â€¢ Detect failures                                                â”‚
    â”‚                                                                         â”‚
    â”‚  2. NOTIFICATION                                                       â”‚
    â”‚     â€¢ Alert administrators                                           â”‚
    â”‚     â€¢ Publish events via Pub/Sub                                    â”‚
    â”‚                                                                         â”‚
    â”‚  3. AUTOMATIC FAILOVER                                                 â”‚
    â”‚     â€¢ If master fails, promote replica                              â”‚
    â”‚     â€¢ Update configuration                                           â”‚
    â”‚     â€¢ Notify clients of new master                                  â”‚
    â”‚                                                                         â”‚
    â”‚  4. CONFIGURATION PROVIDER                                             â”‚
    â”‚     â€¢ Clients ask Sentinel for current master                       â”‚
    â”‚     â€¢ No hardcoded master address in clients                        â”‚
    â”‚                                                                         â”‚
    â”‚  FAILOVER PROCESS:                                                     â”‚
    â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                     â”‚
    â”‚                                                                         â”‚
    â”‚  1. Master stops responding to pings                                 â”‚
    â”‚  2. After timeout, sentinel marks it as SDOWN (subjectively down)   â”‚
    â”‚  3. If quorum sentinels agree, master is ODOWN (objectively down)  â”‚
    â”‚  4. Sentinels vote for a leader to perform failover                 â”‚
    â”‚  5. Leader promotes best replica to master                          â”‚
    â”‚  6. Other replicas reconfigured to follow new master               â”‚
    â”‚                                                                         â”‚
    â”‚  CLIENT CONNECTION:                                                    â”‚
    â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                    â”‚
    â”‚                                                                         â”‚
    â”‚  # Python example with redis-py                                       â”‚
    â”‚  from redis.sentinel import Sentinel                                  â”‚
    â”‚                                                                         â”‚
    â”‚  sentinel = Sentinel([                                                 â”‚
    â”‚      ('sentinel1.example.com', 26379),                               â”‚
    â”‚      ('sentinel2.example.com', 26379),                               â”‚
    â”‚      ('sentinel3.example.com', 26379)                                â”‚
    â”‚  ])                                                                    â”‚
    â”‚                                                                         â”‚
    â”‚  # Get current master                                                 â”‚
    â”‚  master = sentinel.master_for('mymaster')                            â”‚
    â”‚  master.set('key', 'value')                                          â”‚
    â”‚                                                                         â”‚
    â”‚  # Get replica for reads                                              â”‚
    â”‚  replica = sentinel.slave_for('mymaster')                            â”‚
    â”‚  value = replica.get('key')                                          â”‚
    â”‚                                                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


================================================================================
SECTION 2.6: CLUSTER vs SENTINEL COMPARISON
================================================================================

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                                         â”‚
    â”‚  WHEN TO USE WHICH?                                                    â”‚
    â”‚                                                                         â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚                    â”‚  Redis Sentinel    â”‚  Redis Cluster       â”‚  â”‚
    â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
    â”‚  â”‚ Sharding           â”‚  No (single data   â”‚  Yes (automatic)     â”‚  â”‚
    â”‚  â”‚                    â”‚  set)              â”‚                      â”‚  â”‚
    â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
    â”‚  â”‚ Max data size      â”‚  Single node limit â”‚  Unlimited (add      â”‚  â”‚
    â”‚  â”‚                    â”‚  (256GB typ.)      â”‚  more nodes)         â”‚  â”‚
    â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
    â”‚  â”‚ High availability  â”‚  Yes (failover)    â”‚  Yes (built-in)      â”‚  â”‚
    â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
    â”‚  â”‚ Multi-key ops      â”‚  Yes (all keys     â”‚  Only with hash tags â”‚  â”‚
    â”‚  â”‚                    â”‚  on same node)     â”‚                      â”‚  â”‚
    â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
    â”‚  â”‚ Complexity         â”‚  Lower             â”‚  Higher              â”‚  â”‚
    â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
    â”‚  â”‚ Use case           â”‚  <100GB cache,     â”‚  >100GB cache,       â”‚  â”‚
    â”‚  â”‚                    â”‚  simpler ops       â”‚  high throughput     â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â”‚                                                                         â”‚
    â”‚  RECOMMENDATION:                                                       â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                       â”‚
    â”‚  â€¢ Start with Sentinel (simpler)                                     â”‚
    â”‚  â€¢ Move to Cluster when:                                             â”‚
    â”‚    - Data exceeds single node capacity                               â”‚
    â”‚    - Need more than 100K ops/sec                                     â”‚
    â”‚    - Can work around multi-key limitations                          â”‚
    â”‚                                                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


================================================================================
CHAPTER SUMMARY
================================================================================

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                                         â”‚
    â”‚  DISTRIBUTED CACHE - KEY TAKEAWAYS                                    â”‚
    â”‚                                                                         â”‚
    â”‚  DATA DISTRIBUTION                                                     â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                      â”‚
    â”‚  â€¢ Modulo hashing: Simple but bad for scaling                        â”‚
    â”‚  â€¢ Consistent hashing: Only ~1/N keys move on changes               â”‚
    â”‚  â€¢ Virtual nodes: Even distribution                                  â”‚
    â”‚                                                                         â”‚
    â”‚  REPLICATION                                                           â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                           â”‚
    â”‚  â€¢ Leader-follower: Simple, Redis default                           â”‚
    â”‚  â€¢ Async: Fast but potential data loss                              â”‚
    â”‚  â€¢ Sync: Consistent but slower                                       â”‚
    â”‚                                                                         â”‚
    â”‚  REDIS DEPLOYMENT                                                      â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                       â”‚
    â”‚  â€¢ Sentinel: HA without sharding                                     â”‚
    â”‚  â€¢ Cluster: HA with auto-sharding                                   â”‚
    â”‚  â€¢ Hash slots (16384) for distribution                              â”‚
    â”‚                                                                         â”‚
    â”‚  INTERVIEW TIP                                                         â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                         â”‚
    â”‚  Explain consistent hashing with virtual nodes.                      â”‚
    â”‚  Know Redis Cluster vs Sentinel trade-offs.                          â”‚
    â”‚  Discuss replication lag implications.                               â”‚
    â”‚                                                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


================================================================================
                              END OF CHAPTER 2
================================================================================

