================================================================================
         CHAPTER 21: DISTRIBUTED CONCURRENCY CONTROL
         Coordination and Locking in Distributed Systems
================================================================================

In distributed systems, multiple nodes may try to access shared resources
simultaneously. This chapter covers patterns for safe coordination.


================================================================================
SECTION 21.1: THE PROBLEM
================================================================================

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                                         â”‚
    â”‚  WHY DISTRIBUTED CONCURRENCY IS HARD                                   â”‚
    â”‚                                                                         â”‚
    â”‚  SINGLE MACHINE:                                                        â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                       â”‚
    â”‚  mutex.lock()           // OS guarantees exclusivity                   â”‚
    â”‚  critical_section()                                                     â”‚
    â”‚  mutex.unlock()                                                         â”‚
    â”‚                                                                         â”‚
    â”‚  DISTRIBUTED SYSTEM:                                                    â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                    â”‚
    â”‚  â€¢ No shared memory                                                     â”‚
    â”‚  â€¢ Network can fail/delay                                               â”‚
    â”‚  â€¢ Clocks are not synchronized                                         â”‚
    â”‚  â€¢ Nodes can crash                                                      â”‚
    â”‚                                                                         â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚  â”‚                                                                 â”‚   â”‚
    â”‚  â”‚  Server A                    Server B                          â”‚   â”‚
    â”‚  â”‚     â”‚                           â”‚                               â”‚   â”‚
    â”‚  â”‚     â”‚â”€â”€â”€â”€ "acquire lock" â”€â”€â”€â”€â”€â”€â–ºâ”‚ Lock Server                  â”‚   â”‚
    â”‚  â”‚     â”‚                           â”‚ (crashes!)                    â”‚   â”‚
    â”‚  â”‚     â”‚     (no response...)      â”‚                               â”‚   â”‚
    â”‚  â”‚     â”‚                           â”‚                               â”‚   â”‚
    â”‚  â”‚     â”‚  Did I get the lock? ğŸ¤·                                 â”‚   â”‚
    â”‚  â”‚                                                                 â”‚   â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â”‚                                                                         â”‚
    â”‚  SCENARIOS REQUIRING DISTRIBUTED LOCKS:                                â”‚
    â”‚  â€¢ Only one server should process a payment                           â”‚
    â”‚  â€¢ Only one pod should run a cron job                                 â”‚
    â”‚  â€¢ Only one instance should update a resource                         â”‚
    â”‚  â€¢ Leader election                                                     â”‚
    â”‚                                                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


================================================================================
SECTION 21.2: DISTRIBUTED LOCKING WITH REDIS
================================================================================

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                                         â”‚
    â”‚  REDIS SETNX (SET if Not eXists)                                       â”‚
    â”‚                                                                         â”‚
    â”‚  Basic atomic operation for locking:                                   â”‚
    â”‚                                                                         â”‚
    â”‚  SET lock_key unique_value NX PX 30000                                â”‚
    â”‚      â”‚         â”‚           â”‚  â”‚                                         â”‚
    â”‚      â”‚         â”‚           â”‚  â””â”€â”€ Expire in 30 seconds (auto-release) â”‚
    â”‚      â”‚         â”‚           â””â”€â”€ Only set if NOT exists                  â”‚
    â”‚      â”‚         â””â”€â”€ Unique value (UUID) to identify lock owner          â”‚
    â”‚      â””â”€â”€ Key name (e.g., "lock:order:123")                             â”‚
    â”‚                                                                         â”‚
    â”‚  Returns OK if lock acquired, nil if already held                     â”‚
    â”‚                                                                         â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                                                         â”‚
    â”‚  ACQUIRE LOCK:                                                          â”‚
    â”‚                                                                         â”‚
    â”‚  def acquire_lock(lock_key, ttl_ms=30000):                             â”‚
    â”‚      lock_value = str(uuid.uuid4())  # Unique per attempt              â”‚
    â”‚      result = redis.set(                                                â”‚
    â”‚          lock_key,                                                      â”‚
    â”‚          lock_value,                                                    â”‚
    â”‚          nx=True,      # Only if not exists                            â”‚
    â”‚          px=ttl_ms     # Auto-expire                                   â”‚
    â”‚      )                                                                   â”‚
    â”‚      if result:                                                         â”‚
    â”‚          return lock_value  # Success - save this!                     â”‚
    â”‚      return None             # Failed - someone else has it            â”‚
    â”‚                                                                         â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                                                         â”‚
    â”‚  RELEASE LOCK (MUST CHECK OWNERSHIP):                                  â”‚
    â”‚                                                                         â”‚
    â”‚  âœ— WRONG - May release someone else's lock:                           â”‚
    â”‚    redis.delete(lock_key)                                               â”‚
    â”‚                                                                         â”‚
    â”‚  âœ“ CORRECT - Lua script for atomic check-and-delete:                  â”‚
    â”‚                                                                         â”‚
    â”‚  RELEASE_SCRIPT = """                                                   â”‚
    â”‚  if redis.call("GET", KEYS[1]) == ARGV[1] then                         â”‚
    â”‚      return redis.call("DEL", KEYS[1])                                  â”‚
    â”‚  else                                                                    â”‚
    â”‚      return 0                                                            â”‚
    â”‚  end                                                                     â”‚
    â”‚  """                                                                     â”‚
    â”‚                                                                         â”‚
    â”‚  def release_lock(lock_key, lock_value):                               â”‚
    â”‚      return redis.eval(RELEASE_SCRIPT, 1, lock_key, lock_value)        â”‚
    â”‚                                                                         â”‚
    â”‚  WHY LUA? Get + compare + delete must be ATOMIC                        â”‚
    â”‚                                                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


LOCK TTL AND THE SAFETY PROBLEM
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                                         â”‚
    â”‚  THE TTL DILEMMA                                                        â”‚
    â”‚                                                                         â”‚
    â”‚  TTL too short:                                                         â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚  â”‚  Client A acquires lock (TTL=5s)                                â”‚   â”‚
    â”‚  â”‚       â”‚                                                          â”‚   â”‚
    â”‚  â”‚       â”œâ”€â”€â”€â”€ starts work...                                      â”‚   â”‚
    â”‚  â”‚       â”‚     (GC pause / slow network)                           â”‚   â”‚
    â”‚  â”‚       â”‚                                                          â”‚   â”‚
    â”‚  â”‚  [5 seconds pass - LOCK EXPIRES]                                â”‚   â”‚
    â”‚  â”‚                                                                  â”‚   â”‚
    â”‚  â”‚  Client B acquires lock âœ“                                       â”‚   â”‚
    â”‚  â”‚       â”‚                                                          â”‚   â”‚
    â”‚  â”‚       â”œâ”€â”€â”€â”€ starts work...                                      â”‚   â”‚
    â”‚  â”‚       â”‚                                                          â”‚   â”‚
    â”‚  â”‚  Client A resumes (thinks it still has lock!)                   â”‚   â”‚
    â”‚  â”‚       â”‚                                                          â”‚   â”‚
    â”‚  â”‚       â””â”€â”€â”€â”€ modifies resource âœ— CONFLICT!                       â”‚   â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â”‚                                                                         â”‚
    â”‚  TTL too long:                                                          â”‚
    â”‚  â€¢ Client crashes â†’ lock held until TTL expires                       â”‚
    â”‚  â€¢ Other clients blocked unnecessarily                                â”‚
    â”‚                                                                         â”‚
    â”‚  SOLUTIONS:                                                             â”‚
    â”‚  1. Lock renewal (extend TTL while working)                           â”‚
    â”‚  2. Fencing tokens (see below)                                        â”‚
    â”‚                                                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


================================================================================
SECTION 21.3: FENCING TOKENS
================================================================================

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                                         â”‚
    â”‚  FENCING TOKENS - THE DEFINITIVE SOLUTION                              â”‚
    â”‚                                                                         â”‚
    â”‚  Problem: Lock can expire while client thinks it owns it              â”‚
    â”‚  Solution: Include monotonically increasing token with every write    â”‚
    â”‚                                                                         â”‚
    â”‚  HOW IT WORKS:                                                          â”‚
    â”‚                                                                         â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚  â”‚                                                                 â”‚   â”‚
    â”‚  â”‚  Lock Server assigns incrementing token with each lock:        â”‚   â”‚
    â”‚  â”‚                                                                 â”‚   â”‚
    â”‚  â”‚  Client A acquires lock â†’ token = 33                           â”‚   â”‚
    â”‚  â”‚  Client A pauses (GC)                                          â”‚   â”‚
    â”‚  â”‚  Lock expires                                                  â”‚   â”‚
    â”‚  â”‚  Client B acquires lock â†’ token = 34                           â”‚   â”‚
    â”‚  â”‚  Client B writes to storage: "value=X, token=34"               â”‚   â”‚
    â”‚  â”‚  Client A resumes, tries to write: "value=Y, token=33"         â”‚   â”‚
    â”‚  â”‚                                                                 â”‚   â”‚
    â”‚  â”‚  Storage: "33 < 34? REJECT!" âœ—                                 â”‚   â”‚
    â”‚  â”‚                                                                 â”‚   â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â”‚                                                                         â”‚
    â”‚  IMPLEMENTATION:                                                        â”‚
    â”‚                                                                         â”‚
    â”‚  Lock Service:                                                          â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
    â”‚  â”‚ def acquire_lock_with_token(lock_key):                         â”‚     â”‚
    â”‚  â”‚     token = redis.incr("lock_token_counter")  # Atomic incr   â”‚     â”‚
    â”‚  â”‚     acquired = redis.set(lock_key, token, nx=True, px=30000)  â”‚     â”‚
    â”‚  â”‚     if acquired:                                               â”‚     â”‚
    â”‚  â”‚         return token                                           â”‚     â”‚
    â”‚  â”‚     return None                                                 â”‚     â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
    â”‚                                                                         â”‚
    â”‚  Storage/Database:                                                      â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
    â”‚  â”‚ def write(key, value, fence_token):                            â”‚     â”‚
    â”‚  â”‚     current_token = get_token_for_key(key)                     â”‚     â”‚
    â”‚  â”‚     if fence_token < current_token:                            â”‚     â”‚
    â”‚  â”‚         raise StaleTokenError("Rejected: old token")          â”‚     â”‚
    â”‚  â”‚     save(key, value, fence_token)                              â”‚     â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
    â”‚                                                                         â”‚
    â”‚  REQUIREMENT: Storage must support token comparison                   â”‚
    â”‚  (Not all systems do - may need application-level check)              â”‚
    â”‚                                                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


================================================================================
SECTION 21.4: REDLOCK ALGORITHM
================================================================================

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                                         â”‚
    â”‚  SINGLE REDIS = SINGLE POINT OF FAILURE                                â”‚
    â”‚                                                                         â”‚
    â”‚  If Redis master crashes, lock is lost.                                â”‚
    â”‚  Redis replication is asynchronous - failover may lose lock.          â”‚
    â”‚                                                                         â”‚
    â”‚  REDLOCK: Lock across N independent Redis instances (N=5 recommended) â”‚
    â”‚                                                                         â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚  â”‚                                                                 â”‚   â”‚
    â”‚  â”‚              Client wants to acquire lock                      â”‚   â”‚
    â”‚  â”‚                         â”‚                                       â”‚   â”‚
    â”‚  â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚   â”‚
    â”‚  â”‚      â–¼          â–¼       â–¼       â–¼          â–¼                   â”‚   â”‚
    â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”           â”‚   â”‚
    â”‚  â”‚  â”‚Redis 1â”‚ â”‚Redis 2â”‚ â”‚Redis 3â”‚ â”‚Redis 4â”‚ â”‚Redis 5â”‚           â”‚   â”‚
    â”‚  â”‚  â”‚  âœ“    â”‚ â”‚  âœ“    â”‚ â”‚  âœ“    â”‚ â”‚  âœ—    â”‚ â”‚  âœ“    â”‚           â”‚   â”‚
    â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚   â”‚
    â”‚  â”‚                                                                 â”‚   â”‚
    â”‚  â”‚  Got 4/5 = majority â†’ LOCK ACQUIRED âœ“                         â”‚   â”‚
    â”‚  â”‚                                                                 â”‚   â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â”‚                                                                         â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                                                         â”‚
    â”‚  REDLOCK ALGORITHM:                                                     â”‚
    â”‚                                                                         â”‚
    â”‚  1. Get current time (T1)                                              â”‚
    â”‚                                                                         â”‚
    â”‚  2. Try to acquire lock on ALL N Redis instances sequentially         â”‚
    â”‚     â€¢ Same key, same random value, same TTL                           â”‚
    â”‚     â€¢ Small timeout per instance (few ms)                             â”‚
    â”‚                                                                         â”‚
    â”‚  3. Get current time (T2)                                              â”‚
    â”‚     Calculate: elapsed = T2 - T1                                      â”‚
    â”‚     Calculate: validity = TTL - elapsed                               â”‚
    â”‚                                                                         â”‚
    â”‚  4. Lock acquired IF:                                                   â”‚
    â”‚     â€¢ Got lock on majority (N/2 + 1) instances                        â”‚
    â”‚     â€¢ validity > 0 (still time left)                                  â”‚
    â”‚                                                                         â”‚
    â”‚  5. If failed: Release lock on ALL instances                          â”‚
    â”‚                                                                         â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                                                         â”‚
    â”‚  REDLOCK CONTROVERSY (Martin Kleppmann's Critique):                   â”‚
    â”‚                                                                         â”‚
    â”‚  â€¢ Clock drift between nodes can break safety                         â”‚
    â”‚  â€¢ GC pauses can still cause issues                                   â”‚
    â”‚  â€¢ Adds complexity without solving fundamental problems               â”‚
    â”‚                                                                         â”‚
    â”‚  RECOMMENDATION:                                                        â”‚
    â”‚  â€¢ For efficiency (prevent duplicate work): Single Redis is fine      â”‚
    â”‚  â€¢ For correctness (critical data): Use fencing tokens OR             â”‚
    â”‚    consensus systems (ZooKeeper, etcd)                                â”‚
    â”‚                                                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


================================================================================
SECTION 21.5: CONSENSUS-BASED LOCKING (ZOOKEEPER/ETCD)
================================================================================

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                                         â”‚
    â”‚  ZOOKEEPER DISTRIBUTED LOCKS                                           â”‚
    â”‚                                                                         â”‚
    â”‚  ZooKeeper uses Zab consensus protocol - stronger guarantees          â”‚
    â”‚                                                                         â”‚
    â”‚  EPHEMERAL NODES:                                                       â”‚
    â”‚  â€¢ Node is deleted when client session ends                           â”‚
    â”‚  â€¢ Client crash â†’ session timeout â†’ lock auto-released               â”‚
    â”‚                                                                         â”‚
    â”‚  SEQUENTIAL NODES:                                                      â”‚
    â”‚  â€¢ ZooKeeper appends incrementing number to node name                 â”‚
    â”‚  â€¢ Built-in ordering for fair queuing                                 â”‚
    â”‚                                                                         â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚  â”‚                                                                 â”‚   â”‚
    â”‚  â”‚  /locks/my-resource/                                           â”‚   â”‚
    â”‚  â”‚      â”œâ”€â”€ lock-0000000001  (Client A) â† Holds lock              â”‚   â”‚
    â”‚  â”‚      â”œâ”€â”€ lock-0000000002  (Client B) â† Waiting                 â”‚   â”‚
    â”‚  â”‚      â””â”€â”€ lock-0000000003  (Client C) â† Waiting                 â”‚   â”‚
    â”‚  â”‚                                                                 â”‚   â”‚
    â”‚  â”‚  Client B watches lock-0000000001                              â”‚   â”‚
    â”‚  â”‚  When it's deleted â†’ Client B gets lock                        â”‚   â”‚
    â”‚  â”‚                                                                 â”‚   â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â”‚                                                                         â”‚
    â”‚  ALGORITHM:                                                             â”‚
    â”‚  1. Create ephemeral sequential node under /locks/resource/           â”‚
    â”‚  2. Get all children, sort by sequence number                         â”‚
    â”‚  3. If my node is lowest â†’ I have the lock                           â”‚
    â”‚  4. Else â†’ Watch the node just before mine                           â”‚
    â”‚  5. When notified â†’ Go to step 2                                      â”‚
    â”‚                                                                         â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                                                         â”‚
    â”‚  ETCD DISTRIBUTED LOCKS                                                â”‚
    â”‚                                                                         â”‚
    â”‚  etcd uses Raft consensus - similar guarantees to ZooKeeper           â”‚
    â”‚                                                                         â”‚
    â”‚  LEASE-BASED LOCKING:                                                   â”‚
    â”‚  â€¢ Create a lease (like TTL)                                          â”‚
    â”‚  â€¢ Attach key to lease                                                 â”‚
    â”‚  â€¢ Keep-alive to refresh lease                                        â”‚
    â”‚  â€¢ Lease expires â†’ key deleted â†’ lock released                       â”‚
    â”‚                                                                         â”‚
    â”‚  # Using etcd's built-in lock                                          â”‚
    â”‚  etcdctl lock my-lock-name                                              â”‚
    â”‚  # Lock acquired, runs until process exits                            â”‚
    â”‚                                                                         â”‚
    â”‚  # Programmatic                                                         â”‚
    â”‚  lease = client.lease(ttl=30)                                          â”‚
    â”‚  client.put('/locks/my-resource', 'owner-id', lease=lease)            â”‚
    â”‚  lease.refresh()  # Call periodically to keep lock                    â”‚
    â”‚                                                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


================================================================================
SECTION 21.6: LEADER ELECTION
================================================================================

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                                         â”‚
    â”‚  LEADER ELECTION = SPECIAL CASE OF DISTRIBUTED LOCKING                â”‚
    â”‚                                                                         â”‚
    â”‚  Only ONE node should be "leader" at a time (active-passive)          â”‚
    â”‚                                                                         â”‚
    â”‚  USE CASES:                                                             â”‚
    â”‚  â€¢ Job scheduler (only leader schedules jobs)                         â”‚
    â”‚  â€¢ Database master (only leader accepts writes)                       â”‚
    â”‚  â€¢ Cron runner (only leader runs cron tasks)                          â”‚
    â”‚                                                                         â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚  â”‚                                                                 â”‚   â”‚
    â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚   â”‚
    â”‚  â”‚  â”‚ Node A  â”‚  â”‚ Node B  â”‚  â”‚ Node C  â”‚                        â”‚   â”‚
    â”‚  â”‚  â”‚ LEADER  â”‚  â”‚ followerâ”‚  â”‚ followerâ”‚                        â”‚   â”‚
    â”‚  â”‚  â”‚   â˜…     â”‚  â”‚         â”‚  â”‚         â”‚                        â”‚   â”‚
    â”‚  â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                        â”‚   â”‚
    â”‚  â”‚       â”‚            â”‚            â”‚                               â”‚   â”‚
    â”‚  â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚   â”‚
    â”‚  â”‚                    â”‚                                             â”‚   â”‚
    â”‚  â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚   â”‚
    â”‚  â”‚            â”‚   ZooKeeper/  â”‚                                    â”‚   â”‚
    â”‚  â”‚            â”‚     etcd      â”‚                                    â”‚   â”‚
    â”‚  â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚   â”‚
    â”‚  â”‚                                                                 â”‚   â”‚
    â”‚  â”‚  All nodes try to acquire leadership                          â”‚   â”‚
    â”‚  â”‚  Only one succeeds â†’ becomes leader                           â”‚   â”‚
    â”‚  â”‚  Leader fails â†’ another takes over                            â”‚   â”‚
    â”‚  â”‚                                                                 â”‚   â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â”‚                                                                         â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                                                         â”‚
    â”‚  KUBERNETES LEADER ELECTION:                                           â”‚
    â”‚                                                                         â”‚
    â”‚  Uses ConfigMap or Lease object for coordination:                     â”‚
    â”‚                                                                         â”‚
    â”‚  apiVersion: coordination.k8s.io/v1                                    â”‚
    â”‚  kind: Lease                                                            â”‚
    â”‚  metadata:                                                              â”‚
    â”‚    name: my-app-leader                                                  â”‚
    â”‚  spec:                                                                  â”‚
    â”‚    holderIdentity: "pod-abc123"                                        â”‚
    â”‚    leaseDurationSeconds: 15                                            â”‚
    â”‚    renewTime: "2024-01-15T10:30:00Z"                                   â”‚
    â”‚                                                                         â”‚
    â”‚  client-go provides leader election library:                          â”‚
    â”‚  â€¢ Pods compete for Lease ownership                                   â”‚
    â”‚  â€¢ Leader renews lease periodically                                   â”‚
    â”‚  â€¢ If leader fails to renew â†’ another pod takes over                 â”‚
    â”‚                                                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


================================================================================
SECTION 21.7: COMPARISON AND DECISION GUIDE
================================================================================

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                                         â”‚
    â”‚  DISTRIBUTED LOCKING OPTIONS                                           â”‚
    â”‚                                                                         â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚ Approach       â”‚ When to Use                                     â”‚  â”‚
    â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
    â”‚  â”‚ Redis SETNX    â”‚ Simple cases, efficiency lock (not safety)     â”‚  â”‚
    â”‚  â”‚                â”‚ Already have Redis, low latency needed         â”‚  â”‚
    â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
    â”‚  â”‚ Redlock        â”‚ Need higher availability than single Redis     â”‚  â”‚
    â”‚  â”‚                â”‚ Willing to accept complexity                   â”‚  â”‚
    â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
    â”‚  â”‚ ZooKeeper      â”‚ Need strong correctness guarantees             â”‚  â”‚
    â”‚  â”‚                â”‚ Already have ZK (Kafka, Hadoop ecosystem)      â”‚  â”‚
    â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
    â”‚  â”‚ etcd           â”‚ Kubernetes environments                        â”‚  â”‚
    â”‚  â”‚                â”‚ Need strong consistency                        â”‚  â”‚
    â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
    â”‚  â”‚ Database Lock  â”‚ Simple cases, no extra infrastructure          â”‚  â”‚
    â”‚  â”‚                â”‚ SELECT FOR UPDATE or advisory locks            â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â”‚                                                                         â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                                                         â”‚
    â”‚  TWO TYPES OF LOCKS (Martin Kleppmann):                               â”‚
    â”‚                                                                         â”‚
    â”‚  1. EFFICIENCY LOCK                                                     â”‚
    â”‚     Purpose: Avoid duplicate work (e.g., double-sending email)        â”‚
    â”‚     Consequence of failure: Wasted work, minor inconvenience          â”‚
    â”‚     Solution: Simple Redis lock is fine                               â”‚
    â”‚                                                                         â”‚
    â”‚  2. CORRECTNESS LOCK                                                    â”‚
    â”‚     Purpose: Prevent data corruption (e.g., double-charging)          â”‚
    â”‚     Consequence of failure: Data loss, financial loss                 â”‚
    â”‚     Solution: Consensus system + fencing tokens                       â”‚
    â”‚                                                                         â”‚
    â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
    â”‚                                                                         â”‚
    â”‚  INTERVIEW ANSWER FRAMEWORK:                                           â”‚
    â”‚                                                                         â”‚
    â”‚  "For distributed locking, I'd consider:                              â”‚
    â”‚                                                                         â”‚
    â”‚   1. Is this for efficiency or correctness?                           â”‚
    â”‚      - Efficiency: Redis SETNX with TTL                               â”‚
    â”‚      - Correctness: Add fencing tokens                                â”‚
    â”‚                                                                         â”‚
    â”‚   2. What infrastructure do we have?                                  â”‚
    â”‚      - Already have Redis â†’ use Redis                                 â”‚
    â”‚      - Kubernetes â†’ use etcd/Lease                                    â”‚
    â”‚      - Need strong guarantees â†’ ZooKeeper/etcd                       â”‚
    â”‚                                                                         â”‚
    â”‚   3. Always implement:                                                 â”‚
    â”‚      - TTL for auto-release                                           â”‚
    â”‚      - Unique owner ID (prevent wrong release)                        â”‚
    â”‚      - Retry with backoff                                             â”‚
    â”‚      - Fencing tokens for critical operations"                        â”‚
    â”‚                                                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


================================================================================
SECTION 21.8: QUICK REFERENCE
================================================================================

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                                         â”‚
    â”‚  DISTRIBUTED CONCURRENCY CONTROL - CHEAT SHEET                        â”‚
    â”‚                                                                         â”‚
    â”‚  REDIS SETNX PATTERN:                                                   â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                  â”‚
    â”‚  Acquire: SET lock:key uuid NX PX 30000                               â”‚
    â”‚  Release: Lua script (check owner before delete)                      â”‚
    â”‚  Risk: Lock expires while holding â†’ use fencing tokens               â”‚
    â”‚                                                                         â”‚
    â”‚  FENCING TOKEN:                                                         â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                        â”‚
    â”‚  Lock server returns incrementing token                               â”‚
    â”‚  Storage rejects writes with stale tokens                             â”‚
    â”‚  Solves: "zombie lock holder" problem                                 â”‚
    â”‚                                                                         â”‚
    â”‚  REDLOCK:                                                               â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€                                                                â”‚
    â”‚  Lock across N (5) independent Redis instances                        â”‚
    â”‚  Need majority (3) to acquire                                         â”‚
    â”‚  Controversial - use for efficiency, not correctness                  â”‚
    â”‚                                                                         â”‚
    â”‚  ZOOKEEPER/ETCD:                                                        â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                        â”‚
    â”‚  Consensus-based, stronger guarantees                                 â”‚
    â”‚  Ephemeral nodes (ZK) / Leases (etcd)                                 â”‚
    â”‚  Use for correctness-critical locks                                   â”‚
    â”‚                                                                         â”‚
    â”‚  LEADER ELECTION:                                                       â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                       â”‚
    â”‚  Special case of distributed lock                                     â”‚
    â”‚  Only one leader at a time                                            â”‚
    â”‚  K8s: Use Lease object + client-go library                           â”‚
    â”‚                                                                         â”‚
    â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
    â”‚                                                                         â”‚
    â”‚  KEY TAKEAWAYS:                                                         â”‚
    â”‚                                                                         â”‚
    â”‚  â€¢ No perfect distributed lock exists                                 â”‚
    â”‚  â€¢ Always use TTL (prevent deadlocks)                                 â”‚
    â”‚  â€¢ Use unique owner ID (prevent wrong release)                        â”‚
    â”‚  â€¢ Fencing tokens for correctness                                     â”‚
    â”‚  â€¢ Choose based on: efficiency vs correctness requirement             â”‚
    â”‚                                                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


================================================================================
                              END OF CHAPTER 21
================================================================================
